{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### type 2 dembedding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ce code dynamique est conçu pour gérer la classification de tous les types d'erreurs spécifiés dans le dataset. L'utilisateur peut sélectionner le type d'erreur à analyser directement via une saisie dans le terminal, ce qui déclenche un pipeline de nettoyage et de traitement adapté à ce type d'erreur. Le pipeline applique des étapes de prétraitement personnalisées, génère des embeddings vectoriels à l'aide de bge-m3, calcule une mesure de similarité entre les phrases source et simplifiées, et effectue une classification supervisée en utilisant la régression logistique. Cette approche flexible et automatisée permet de traiter chaque type d'erreur de manière spécifique et d'assurer un apprentissage performant tout en s'adaptant aux besoins de l'utilisateur."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Voici les types d'erreurs disponibles :\n",
      "1. Random generation\n",
      "2. Syntax error\n",
      "3. Contradiction\n",
      "4. Simple punctuation / grammar errors\n",
      "5. Redundancy\n",
      "6. Format misalignement\n",
      "7. Prompt misalignement\n",
      "8. Out-of-Scope Generation\n",
      "9. Topic shift\n",
      "10. Oversimplification of Logical Arguments\n",
      "11. Overgeneralization\n",
      "12. Loss of Informative Content\n",
      "13. Factuality hallucination\n",
      "14. Faithfulness hallucination\n",
      "Vous avez choisi : Out-of-Scope Generation\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bddf0744955a4ef39eafc2611a856291",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 30 files:   0%|          | 0/30 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Confusion Matrix de Logistic Regression:\n",
      "[[48  8]\n",
      " [ 0 10]]\n",
      "\n",
      "Classification Report de Logistic Regression:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       False       1.00      0.86      0.92        56\n",
      "        True       0.56      1.00      0.71        10\n",
      "\n",
      "    accuracy                           0.88        66\n",
      "   macro avg       0.78      0.93      0.82        66\n",
      "weighted avg       0.93      0.88      0.89        66\n",
      "\n",
      "Balanced Accuracy de Logistic Regression: 0.929\n",
      "\n",
      "Confusion Matrix de SVM:\n",
      "[[51  5]\n",
      " [ 3  7]]\n",
      "\n",
      "Classification Report de SVM:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       False       0.94      0.91      0.93        56\n",
      "        True       0.58      0.70      0.64        10\n",
      "\n",
      "    accuracy                           0.88        66\n",
      "   macro avg       0.76      0.81      0.78        66\n",
      "weighted avg       0.89      0.88      0.88        66\n",
      "\n",
      "Balanced Accuracy de SVM: 0.805\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import classification_report, confusion_matrix, balanced_accuracy_score\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from FlagEmbedding import BGEM3FlagModel\n",
    "import spacy\n",
    "\n",
    "data=pd.read_csv(\"fusionned_file.csv\",sep=\";\")\n",
    "data.drop(columns=['run_id', 'snt_id','No error' ,'Commentaire'], inplace=True)  \n",
    "\n",
    "# Charger SpaCy pour le traitement du langage\n",
    "nlp = spacy.load(\"en_core_web_lg\")\n",
    "\n",
    "# Fonction pour supprimer la ponctuation\n",
    "def remove_punctuation(text):\n",
    "    return \"\".join([char for char in text if char.isalnum() or char.isspace()])\n",
    "\n",
    "# Fonction pour convertir en minuscules\n",
    "def to_lowercase(text):\n",
    "    return text.lower()\n",
    "\n",
    "# Fonction pour supprimer les stopwords\n",
    "def remove_stopwords(text):\n",
    "    doc = nlp(text)\n",
    "    return \" \".join([token.text for token in doc if not token.is_stop])\n",
    "\n",
    "# Fonction pour effectuer la lemmatisation\n",
    "def lemmatize(text):\n",
    "    doc = nlp(text)\n",
    "    return \" \".join([token.lemma_ for token in doc])\n",
    "\n",
    "# Pipeline configurable\n",
    "def clean_text(text, config):\n",
    "    if config.get(\"remove_punctuation\", False):\n",
    "        text = remove_punctuation(text)\n",
    "    if config.get(\"to_lowercase\", False):\n",
    "        text = to_lowercase(text)\n",
    "    if config.get(\"remove_stopwords\", False):\n",
    "        text = remove_stopwords(text)\n",
    "    if config.get(\"lemmatize\", False):\n",
    "        text = lemmatize(text)\n",
    "    return text\n",
    "\n",
    "# Configuration du pipeline pour différents types d’erreurs\n",
    "pipeline_configs = {\n",
    "    \"Random generation\": {\n",
    "        \"remove_punctuation\": True,     # Les phrases générées aléatoirement peuvent inclure beaucoup de ponctuation inutile.\n",
    "        \"to_lowercase\": True,           # Uniformiser les cas pour éviter les variations inutiles.\n",
    "        \"remove_stopwords\": True,       # Les stopwords n'apportent aucune information pertinente.\n",
    "        \"lemmatize\": True,              # Simplifier les mots générés aléatoirement.\n",
    "    },\n",
    "    \"Syntax error\": {\n",
    "        \"remove_punctuation\": False,    # Garder la ponctuation pour identifier les erreurs syntaxiques.\n",
    "        \"to_lowercase\": True,           # Uniformiser les textes pour une analyse syntaxique cohérente.\n",
    "        \"remove_stopwords\": False,      # Les stopwords peuvent être nécessaires pour détecter les erreurs syntaxiques.\n",
    "        \"lemmatize\": False,             # Éviter de modifier les structures pour préserver l'intégrité syntaxique.\n",
    "    },\n",
    "    \"Contradiction\": {\n",
    "        \"remove_punctuation\": True,     # Supprimer la ponctuation pour se concentrer sur les contradictions sémantiques.\n",
    "        \"to_lowercase\": True,\n",
    "        \"remove_stopwords\": True,       # Les stopwords n'ont pas d'impact sur les contradictions.\n",
    "        \"lemmatize\": True,              # Faciliter la détection de contradictions.\n",
    "    },\n",
    "    \"Simple punctuation / grammar errors\": {\n",
    "        \"remove_punctuation\": False,    # Garder la ponctuation pour détecter les erreurs de ponctuation.\n",
    "        \"to_lowercase\": True,           # Uniformiser les cas.\n",
    "        \"remove_stopwords\": False,      # Conserver les stopwords pour ne pas altérer les phrases.\n",
    "        \"lemmatize\": False,             # Ne pas modifier les formes des mots.\n",
    "    },\n",
    "    \"Redundancy\": {\n",
    "        \"remove_punctuation\": True,     # Supprimer les ponctuations inutiles pour analyser la redondance.\n",
    "        \"to_lowercase\": True,           \n",
    "        \"remove_stopwords\": False,      # Conserver les stopwords pour identifier des répétitions complètes.\n",
    "        \"lemmatize\": False,             # Ne pas modifier les mots pour détecter les répétitions exactes.\n",
    "    },\n",
    "    \"Format misalignement\": {\n",
    "        \"remove_punctuation\": False,    # Garder la ponctuation pour vérifier les erreurs de format.\n",
    "        \"to_lowercase\": True,           \n",
    "        \"remove_stopwords\": False,      # Les stopwords peuvent être utiles pour détecter des problèmes de formatage.\n",
    "        \"lemmatize\": False,             \n",
    "    },\n",
    "    \"Prompt misalignement\": {\n",
    "        \"remove_punctuation\": False,     # Supprimer les caractères inutiles pour vérifier la correspondance avec le prompt.\n",
    "        \"to_lowercase\": True,\n",
    "        \"remove_stopwords\": True,       # Nettoyer les phrases pour se concentrer sur la correspondance avec le prompt.\n",
    "        \"lemmatize\": True,              \n",
    "    },\n",
    "    \"Out-of-Scope Generation\": {\n",
    "        \"remove_punctuation\": False,    # Garder la ponctuation pour analyser les phrases hors contexte.\n",
    "        \"to_lowercase\": True,           \n",
    "        \"remove_stopwords\": False,      \n",
    "        \"lemmatize\": True,              \n",
    "    },\n",
    "    \"Topic shift\": {\n",
    "        \"remove_punctuation\": False,   \n",
    "        \"to_lowercase\": True,\n",
    "        \"remove_stopwords\": False,    \n",
    "        \"lemmatize\": True,              \n",
    "    },\n",
    "    \"Oversimplification of Logical Arguments\": {\n",
    "        \"remove_punctuation\": True,     # Supprimer la ponctuation pour clarifier les arguments.\n",
    "        \"to_lowercase\": True,\n",
    "        \"remove_stopwords\": False,       \n",
    "        \"lemmatize\": False,              \n",
    "    },\n",
    "    \"Overgeneralization\": {\n",
    "        \"remove_punctuation\": True,     # Supprimer la ponctuation pour simplifier les phrases trop généralisées.\n",
    "        \"to_lowercase\": True,\n",
    "        \"remove_stopwords\": False,       \n",
    "        \"lemmatize\": False,              \n",
    "    },\n",
    "    \"Loss of Informative Content\": {\n",
    "        \"remove_punctuation\": True,     # Supprimer les ponctuations inutiles pour clarifier les informations perdues.\n",
    "        \"to_lowercase\": True,\n",
    "        \"remove_stopwords\": False,       \n",
    "        \"lemmatize\": False,              \n",
    "    },\n",
    "    \"Factuality hallucination\": {\n",
    "        \"remove_punctuation\": True,     # Supprimer les ponctuations inutiles pour analyser les faits.\n",
    "        \"to_lowercase\": True,\n",
    "        \"remove_stopwords\": False,       \n",
    "        \"lemmatize\": True,              \n",
    "    },\n",
    "    \"Faithfulness hallucination\": {\n",
    "        \"remove_punctuation\": True,     # Supprimer la ponctuation pour se concentrer sur la fidélité.\n",
    "        \"to_lowercase\": True,\n",
    "        \"remove_stopwords\": False,       \n",
    "        \"lemmatize\": True,              \n",
    "    },\n",
    "}\n",
    "\n",
    "# Liste des colonnes d'erreurs\n",
    "error_columns = [\n",
    "    'Random generation', 'Syntax error', 'Contradiction',\n",
    "    'Simple punctuation / grammar errors', 'Redundancy', 'Format misalignement',\n",
    "    'Prompt misalignement', 'Out-of-Scope Generation', 'Topic shift',\n",
    "    'Oversimplification of Logical Arguments', 'Overgeneralization',\n",
    "    'Loss of Informative Content', 'Factuality hallucination', 'Faithfulness hallucination'\n",
    "]\n",
    "\n",
    "# Demander à l'utilisateur de choisir une colonne cible\n",
    "print(\"Voici les types d'erreurs disponibles :\")\n",
    "for i, col in enumerate(error_columns):\n",
    "    print(f\"{i + 1}. {col}\")\n",
    "\n",
    "choice = int(input(\"Entrez le numéro correspondant à la cible souhaitée : \")) - 1\n",
    "target_column = error_columns[choice]\n",
    "print(f\"Vous avez choisi : {target_column}\")\n",
    "\n",
    "\n",
    "\n",
    "# Définir la colonne cible\n",
    "data['target'] = data[target_column].astype(int)\n",
    "\n",
    "# Appliquer le pipeline de nettoyage\n",
    "config = pipeline_configs.get(target_column, {})\n",
    "data['cleaned_source'] = data['source sentence'].apply(lambda x: clean_text(x, config))\n",
    "data['cleaned_simplified'] = data['simplified sentence'].apply(lambda x: clean_text(x, config))\n",
    "\n",
    "# Charger le modèle BGEM3FlagModel\n",
    "embedding_model = BGEM3FlagModel('BAAI/bge-m3', use_fp16=True)\n",
    "\n",
    "# Générer les embeddings\n",
    "data['source_latent'] = data['cleaned_source'].apply(embedding_model.encode)\n",
    "data['simplified_latent'] = data['cleaned_simplified'].apply(embedding_model.encode)\n",
    "\n",
    "# Extraire les vecteurs denses\n",
    "data['source_embedding'] = data['source_latent'].apply(lambda x: np.array(x['dense_vecs']))\n",
    "data['simplified_embedding'] = data['simplified_latent'].apply(lambda x: np.array(x['dense_vecs']))\n",
    "\n",
    "# Supprimer les colonnes intermédiaires\n",
    "data = data.drop(columns=['source_latent', 'simplified_latent'])\n",
    "\n",
    "# Calculer la similarité cosinus\n",
    "data['similarity'] = data.apply(\n",
    "    lambda row: cosine_similarity([row['source_embedding']], [row['simplified_embedding']])[0][0], axis=1\n",
    ")\n",
    "\n",
    "# Combiner les embeddings et la similarité\n",
    "X = np.array([\n",
    "    np.concatenate([row['source_embedding'], row['simplified_embedding'], [row['similarity']]])\n",
    "    for _, row in data.iterrows()\n",
    "])\n",
    "y = data['target'].values\n",
    "\n",
    "# Diviser les données en ensembles d'entraînement et de test\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Appliquer SMOTE\n",
    "smote = SMOTE(random_state=42)\n",
    "X_train_resampled, y_train_resampled = smote.fit_resample(X_train, y_train)\n",
    "\n",
    "# Entraîner et évaluer Logistic Regression\n",
    "logistic_model = LogisticRegression(max_iter=1000, random_state=42)\n",
    "logistic_model.fit(X_train_resampled, y_train_resampled)\n",
    "y_pred = logistic_model.predict(X_test)\n",
    "\n",
    "print(\"\\nConfusion Matrix de Logistic Regression:\")\n",
    "print(confusion_matrix(y_test, y_pred))\n",
    "print(\"\\nClassification Report de Logistic Regression:\")\n",
    "print(classification_report(y_test, y_pred, target_names=[\"False\", \"True\"]))\n",
    "print(f\"Balanced Accuracy de Logistic Regression: {balanced_accuracy_score(y_test, y_pred):.3f}\")\n",
    "\n",
    "# Entraîner et évaluer SVM\n",
    "svm_model = SVC(kernel='linear', random_state=42)\n",
    "svm_model.fit(X_train_resampled, y_train_resampled)\n",
    "y_pred_svm = svm_model.predict(X_test)\n",
    "\n",
    "print(\"\\nConfusion Matrix de SVM:\")\n",
    "print(confusion_matrix(y_test, y_pred_svm))\n",
    "print(\"\\nClassification Report de SVM:\")\n",
    "print(classification_report(y_test, y_pred_svm, target_names=[\"False\", \"True\"]))\n",
    "print(f\"Balanced Accuracy de SVM: {balanced_accuracy_score(y_test, y_pred_svm):.3f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "L'erreur 'Out-of-Scope Generation' est présente dans cette simplification.\n"
     ]
    }
   ],
   "source": [
    "# Fonction pour prédire si une erreur est présente ou non\n",
    "def predict_error(user_source, user_simplified, logistic_model, config):\n",
    "    # Nettoyer les phrases avec le pipeline configuré\n",
    "    cleaned_source = clean_text(user_source, config)\n",
    "    cleaned_simplified = clean_text(user_simplified, config)\n",
    "\n",
    "    # Générer les embeddings avec le modèle d'embedding\n",
    "    source_embedding = embedding_model.encode(cleaned_source)\n",
    "    simplified_embedding = embedding_model.encode(cleaned_simplified)\n",
    "\n",
    "    # Si les embeddings retournent un dictionnaire, extraire les vecteurs denses\n",
    "    if isinstance(source_embedding, dict) and 'dense_vecs' in source_embedding:\n",
    "        source_embedding = np.array(source_embedding['dense_vecs'])\n",
    "    if isinstance(simplified_embedding, dict) and 'dense_vecs' in simplified_embedding:\n",
    "        simplified_embedding = np.array(simplified_embedding['dense_vecs'])\n",
    "\n",
    "    # Calculer la similarité cosinus\n",
    "    similarity = cosine_similarity([source_embedding], [simplified_embedding])[0][0]\n",
    "\n",
    "    # Combiner les embeddings et la similarité en un seul vecteur\n",
    "    combined_features = np.concatenate([source_embedding, simplified_embedding, [similarity]])\n",
    "\n",
    "    # Prédire à l'aide du modèle entraîné\n",
    "    prediction = logistic_model.predict([combined_features])\n",
    "    return prediction[0]\n",
    "\n",
    "# Demander à l'utilisateur de saisir des phrases\n",
    "user_source = input(\"Entrez la phrase source : \")\n",
    "user_simplified = input(\"Entrez la phrase simplifiée : \")\n",
    "\n",
    "# Prédire le type d'erreur\n",
    "user_prediction = predict_error(user_source, user_simplified, logistic_model, config)\n",
    "\n",
    "# Afficher le résultat\n",
    "if user_prediction == 1:\n",
    "    print(f\"L'erreur '{target_column}' est présente dans cette simplification.\")\n",
    "else:\n",
    "    print(f\"L'erreur '{target_column}' n'est pas présente dans cette simplification.\")\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
