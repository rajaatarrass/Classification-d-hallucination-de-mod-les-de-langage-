{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ce code dynamique est conçu pour gérer la classification de tous les types d'erreurs spécifiés dans le dataset. L'utilisateur peut sélectionner le type d'erreur à analyser directement via une saisie dans le terminal, ce qui déclenche un pipeline de nettoyage et de traitement adapté à ce type d'erreur. Le pipeline applique des étapes de prétraitement personnalisées, génère des embeddings vectoriels à l'aide de Sentence-BERT, calcule une mesure de similarité et effectue une classification supervisée (Logistic Regression ). Cette approche flexible permet de traiter efficacement chaque type d'erreur de manière spécifique et automatisée."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import spacy\n",
    "from unidecode import unidecode\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from sklearn.metrics import balanced_accuracy_score\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "# Charger SpaCy pour le traitement du langage\n",
    "nlp = spacy.load(\"en_core_web_lg\")\n",
    "\n",
    "# Fonction pour supprimer la ponctuation\n",
    "def remove_punctuation(text):\n",
    "    return \"\".join([char for char in text if char.isalnum() or char.isspace()])\n",
    "\n",
    "# Fonction pour convertir en minuscules\n",
    "def to_lowercase(text):\n",
    "    return text.lower()\n",
    "\n",
    "# Fonction pour supprimer les stopwords\n",
    "def remove_stopwords(text):\n",
    "    doc = nlp(text)\n",
    "    return \" \".join([token.text for token in doc if not token.is_stop])\n",
    "\n",
    "# Fonction pour effectuer la lemmatisation\n",
    "def lemmatize(text):\n",
    "    doc = nlp(text)\n",
    "    return \" \".join([token.lemma_ for token in doc])\n",
    "\n",
    "# Pipeline configurable\n",
    "def clean_text(text, config):\n",
    "    \"\"\"\n",
    "    Nettoie le texte selon un pipeline configurable.\n",
    "    :param text: Le texte brut.\n",
    "    :param config: Dictionnaire de configuration pour activer/désactiver les étapes.\n",
    "    :return: Texte nettoyé.\n",
    "    \"\"\"\n",
    "    if config.get(\"remove_punctuation\", False):\n",
    "        text = remove_punctuation(text)\n",
    "    if config.get(\"to_lowercase\", False):\n",
    "        text = to_lowercase(text)\n",
    "    if config.get(\"remove_stopwords\", False):\n",
    "        text = remove_stopwords(text)\n",
    "    if config.get(\"lemmatize\", False):\n",
    "        text = lemmatize(text)\n",
    "    return text\n",
    "\n",
    "# Configuration du pipeline pour différents types d’erreurs\n",
    "pipeline_configs = {\n",
    "    \"Random generation\": {\n",
    "        \"remove_punctuation\": True,     # Les phrases générées aléatoirement peuvent inclure beaucoup de ponctuation inutile.\n",
    "        \"to_lowercase\": True,           # Uniformiser les cas pour éviter les variations inutiles.\n",
    "        \"remove_stopwords\": True,       # Les stopwords n'apportent aucune information pertinente.\n",
    "        \"lemmatize\": True,              # Simplifier les mots générés aléatoirement.\n",
    "    },\n",
    "    \"Syntax error\": {\n",
    "        \"remove_punctuation\": False,    # Garder la ponctuation pour identifier les erreurs syntaxiques.\n",
    "        \"to_lowercase\": True,           # Uniformiser les textes pour une analyse syntaxique cohérente.\n",
    "        \"remove_stopwords\": False,      # Les stopwords peuvent être nécessaires pour détecter les erreurs syntaxiques.\n",
    "        \"lemmatize\": False,             # Éviter de modifier les structures pour préserver l'intégrité syntaxique.\n",
    "    },\n",
    "    \"Contradiction\": {\n",
    "        \"remove_punctuation\": True,     # Supprimer la ponctuation pour se concentrer sur les contradictions sémantiques.\n",
    "        \"to_lowercase\": True,\n",
    "        \"remove_stopwords\": True,       # Les stopwords n'ont pas d'impact sur les contradictions.\n",
    "        \"lemmatize\": True,              # Faciliter la détection de contradictions.\n",
    "    },\n",
    "    \"Simple punctuation / grammar errors\": {\n",
    "        \"remove_punctuation\": False,    # Garder la ponctuation pour détecter les erreurs de ponctuation.\n",
    "        \"to_lowercase\": True,           # Uniformiser les cas.\n",
    "        \"remove_stopwords\": False,      # Conserver les stopwords pour ne pas altérer les phrases.\n",
    "        \"lemmatize\": False,             # Ne pas modifier les formes des mots.\n",
    "    },\n",
    "    \"Redundancy\": {\n",
    "        \"remove_punctuation\": True,     # Supprimer les ponctuations inutiles pour analyser la redondance.\n",
    "        \"to_lowercase\": True,           \n",
    "        \"remove_stopwords\": False,      # Conserver les stopwords pour identifier des répétitions complètes.\n",
    "        \"lemmatize\": False,             # Ne pas modifier les mots pour détecter les répétitions exactes.\n",
    "    },\n",
    "    \"Format misalignement\": {\n",
    "        \"remove_punctuation\": False,    # Garder la ponctuation pour vérifier les erreurs de format.\n",
    "        \"to_lowercase\": True,           \n",
    "        \"remove_stopwords\": False,      # Les stopwords peuvent être utiles pour détecter des problèmes de formatage.\n",
    "        \"lemmatize\": False,             \n",
    "    },\n",
    "    \"Prompt misalignement\": {\n",
    "        \"remove_punctuation\": False,     # Supprimer les caractères inutiles pour vérifier la correspondance avec le prompt.\n",
    "        \"to_lowercase\": True,\n",
    "        \"remove_stopwords\": True,       # Nettoyer les phrases pour se concentrer sur la correspondance avec le prompt.\n",
    "        \"lemmatize\": True,              \n",
    "    },\n",
    "    \"Out-of-Scope Generation\": {\n",
    "        \"remove_punctuation\": False,    # Garder la ponctuation pour analyser les phrases hors contexte.\n",
    "        \"to_lowercase\": True,           \n",
    "        \"remove_stopwords\": False,      \n",
    "        \"lemmatize\": True,              \n",
    "    },\n",
    "    \"Topic shift\": {\n",
    "        \"remove_punctuation\": False,   \n",
    "        \"to_lowercase\": True,\n",
    "        \"remove_stopwords\": False,    \n",
    "        \"lemmatize\": True,              \n",
    "    },\n",
    "    \"Oversimplification of Logical Arguments\": {\n",
    "        \"remove_punctuation\": True,     # Supprimer la ponctuation pour clarifier les arguments.\n",
    "        \"to_lowercase\": True,\n",
    "        \"remove_stopwords\": False,       \n",
    "        \"lemmatize\": False,              \n",
    "    },\n",
    "    \"Overgeneralization\": {\n",
    "        \"remove_punctuation\": True,     # Supprimer la ponctuation pour simplifier les phrases trop généralisées.\n",
    "        \"to_lowercase\": True,\n",
    "        \"remove_stopwords\": False,       \n",
    "        \"lemmatize\": False,              \n",
    "    },\n",
    "    \"Loss of Informative Content\": {\n",
    "        \"remove_punctuation\": True,     # Supprimer les ponctuations inutiles pour clarifier les informations perdues.\n",
    "        \"to_lowercase\": True,\n",
    "        \"remove_stopwords\": False,       \n",
    "        \"lemmatize\": False,              \n",
    "    },\n",
    "    \"Factuality hallucination\": {\n",
    "        \"remove_punctuation\": True,     # Supprimer les ponctuations inutiles pour analyser les faits.\n",
    "        \"to_lowercase\": True,\n",
    "        \"remove_stopwords\": False,       \n",
    "        \"lemmatize\": True,              \n",
    "    },\n",
    "    \"Faithfulness hallucination\": {\n",
    "        \"remove_punctuation\": True,     # Supprimer la ponctuation pour se concentrer sur la fidélité.\n",
    "        \"to_lowercase\": True,\n",
    "        \"remove_stopwords\": False,       \n",
    "        \"lemmatize\": True,              \n",
    "    },\n",
    "}\n",
    "\n",
    "\n",
    "# Liste des colonnes d'erreurs\n",
    "error_columns = [\n",
    "    'Random generation', 'Syntax error', 'Contradiction',\n",
    "    'Simple punctuation / grammar errors', 'Redundancy', 'Format misalignement',\n",
    "    'Prompt misalignement', 'Out-of-Scope Generation', 'Topic shift',\n",
    "    'Oversimplification of Logical Arguments', 'Overgeneralization',\n",
    "    'Loss of Informative Content', 'Factuality hallucination', 'Faithfulness hallucination'\n",
    "]\n",
    "\n",
    "# Demander à l'utilisateur de choisir une colonne cible\n",
    "print(\"Voici les types d'erreurs disponibles :\")\n",
    "for i, col in enumerate(error_columns):\n",
    "    print(f\"{i + 1}. {col}\")\n",
    "\n",
    "choice = int(input(\"Entrez le numéro correspondant à la cible souhaitée : \")) - 1\n",
    "target_column = error_columns[choice]\n",
    "\n",
    "print(f\"Vous avez choisi : {target_column}\")\n",
    "\n",
    "# Définir la colonne cible\n",
    "data['target'] = data[target_column].astype(int)\n",
    "\n",
    "# Appliquer le pipeline de nettoyage\n",
    "config = pipeline_configs[target_column]\n",
    "data['cleaned_source'] = data['source sentence'].apply(lambda x: clean_text(x, config))\n",
    "data['cleaned_simplified'] = data['simplified sentence'].apply(lambda x: clean_text(x, config))\n",
    "\n",
    "# Charger le modèle d'embedding\n",
    "model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "\n",
    "# Générer les embeddings\n",
    "data['source_embedding'] = data['cleaned_source'].apply(model.encode)\n",
    "data['simplified_embedding'] = data['cleaned_simplified'].apply(model.encode)\n",
    "\n",
    "# Calculer la similarité cosinus\n",
    "data['similarity'] = data.apply(\n",
    "    lambda row: cosine_similarity([row['source_embedding']], [row['simplified_embedding']])[0][0], axis=1\n",
    ")\n",
    "\n",
    "# Combiner les embeddings et la similarité\n",
    "X = np.array([\n",
    "    np.concatenate([row['source_embedding'], row['simplified_embedding'], [row['similarity']]])\n",
    "    for _, row in data.iterrows()\n",
    "])\n",
    "y = data['target'].values\n",
    "\n",
    "# Diviser les données en ensembles d'entraînement et de test\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "smote = SMOTE(random_state=42)\n",
    "X_train_resampled, y_train_resampled = smote.fit_resample(X_train, y_train)\n",
    "\n",
    "# Entraîner un modèle (Logistic Regression par exemple)\n",
    "model = LogisticRegression(max_iter=1000, random_state=42)\n",
    "model.fit(X_train_resampled, y_train_resampled)\n",
    "\n",
    "# Prédictions\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# Évaluation des performances\n",
    "print(\"\\nConfusion Matrix of logistic regression:\")\n",
    "print(confusion_matrix(y_test, y_pred))\n",
    "\n",
    "print(\"\\nClassification Report of logistic regression  :\")\n",
    "print(classification_report(y_test, y_pred, target_names=[\"False\", \"True\"]))\n",
    "print(f\"The balanced accuracy of the model logistic regression is \"\n",
    "      f\"{balanced_accuracy_score(y_test, y_pred):.3f}\")\n",
    "\n",
    "\n",
    "svm_model = SVC(kernel='linear', random_state=42)\n",
    "svm_model.fit(X_train_resampled, y_train_resampled)\n",
    "y_pred = svm_model.predict(X_test)\n",
    "print(\"Confusion Matrix of the model SVM :\")\n",
    "print(confusion_matrix(y_test, y_pred))\n",
    "\n",
    "print(\"\\nClassification Report of the model SVM :\")\n",
    "print(classification_report(y_test, y_pred, target_names=[\"False\", \"True\"]))\n",
    "print(f\"The balanced accuracy of the model SVM  is  \"\n",
    "      f\"{balanced_accuracy_score(y_test, y_pred):.3f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
