{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### type 1 dembedding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ce code dynamique est conçu pour gérer la classification de tous les types d'erreurs spécifiés dans le dataset. L'utilisateur peut sélectionner le type d'erreur à analyser directement via une saisie dans le terminal, ce qui déclenche un pipeline de nettoyage et de traitement adapté à ce type d'erreur. Le pipeline applique des étapes de prétraitement personnalisées, génère des embeddings vectoriels à l'aide de Sentence-BERT, calcule une mesure de similarité et effectue une classification supervisée (Logistic Regression ). Cette approche flexible permet de traiter efficacement chaque type d'erreur de manière spécifique et automatisée."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Voici les types d'erreurs disponibles :\n",
      "1. Random generation\n",
      "2. Syntax error\n",
      "3. Contradiction\n",
      "4. Simple punctuation / grammar errors\n",
      "5. Redundancy\n",
      "6. Format misalignement\n",
      "7. Prompt misalignement\n",
      "8. Out-of-Scope Generation\n",
      "9. Topic shift\n",
      "10. Oversimplification of Logical Arguments\n",
      "11. Overgeneralization\n",
      "12. Loss of Informative Content\n",
      "13. Factuality hallucination\n",
      "14. Faithfulness hallucination\n",
      "Vous avez choisi : Out-of-Scope Generation\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\hp\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\transformers\\tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Confusion Matrix of logistic regression:\n",
      "[[45 11]\n",
      " [ 3  7]]\n",
      "\n",
      "Classification Report of logistic regression  :\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       False       0.94      0.80      0.87        56\n",
      "        True       0.39      0.70      0.50        10\n",
      "\n",
      "    accuracy                           0.79        66\n",
      "   macro avg       0.66      0.75      0.68        66\n",
      "weighted avg       0.85      0.79      0.81        66\n",
      "\n",
      "The balanced accuracy of the logistic_model logistic regression is 0.752\n",
      "Confusion Matrix of the model SVM :\n",
      "[[50  6]\n",
      " [ 7  3]]\n",
      "\n",
      "Classification Report of the model SVM :\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       False       0.88      0.89      0.88        56\n",
      "        True       0.33      0.30      0.32        10\n",
      "\n",
      "    accuracy                           0.80        66\n",
      "   macro avg       0.61      0.60      0.60        66\n",
      "weighted avg       0.79      0.80      0.80        66\n",
      "\n",
      "The balanced accuracy of the model SVM  is  0.596\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import spacy\n",
    "from unidecode import unidecode\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from sklearn.metrics import balanced_accuracy_score\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "data=pd.read_csv(\"fusionned_file.csv\",sep=\";\")\n",
    "data.drop(columns=['run_id', 'snt_id','No error' ,'Commentaire'], inplace=True)\n",
    "\n",
    "# Charger SpaCy pour le traitement du langage\n",
    "nlp = spacy.load(\"en_core_web_lg\")\n",
    "\n",
    "# Fonction pour supprimer la ponctuation\n",
    "def remove_punctuation(text):\n",
    "    return \"\".join([char for char in text if char.isalnum() or char.isspace()])\n",
    "\n",
    "# Fonction pour convertir en minuscules\n",
    "def to_lowercase(text):\n",
    "    return text.lower()\n",
    "\n",
    "# Fonction pour supprimer les stopwords\n",
    "def remove_stopwords(text):\n",
    "    doc = nlp(text)\n",
    "    return \" \".join([token.text for token in doc if not token.is_stop])\n",
    "\n",
    "# Fonction pour effectuer la lemmatisation\n",
    "def lemmatize(text):\n",
    "    doc = nlp(text)\n",
    "    return \" \".join([token.lemma_ for token in doc])\n",
    "\n",
    "# Pipeline configurable\n",
    "def clean_text(text, config):\n",
    "    \"\"\"\n",
    "    Nettoie le texte selon un pipeline configurable.\n",
    "    :param text: Le texte brut.\n",
    "    :param config: Dictionnaire de configuration pour activer/désactiver les étapes.\n",
    "    :return: Texte nettoyé.\n",
    "    \"\"\"\n",
    "    if config.get(\"remove_punctuation\", False):\n",
    "        text = remove_punctuation(text)\n",
    "    if config.get(\"to_lowercase\", False):\n",
    "        text = to_lowercase(text)\n",
    "    if config.get(\"remove_stopwords\", False):\n",
    "        text = remove_stopwords(text)\n",
    "    if config.get(\"lemmatize\", False):\n",
    "        text = lemmatize(text)\n",
    "    return text\n",
    "\n",
    "# Configuration du pipeline pour différents types d’erreurs\n",
    "pipeline_configs = {\n",
    "    \"Random generation\": {\n",
    "        \"remove_punctuation\": True,     # Les phrases générées aléatoirement peuvent inclure beaucoup de ponctuation inutile.\n",
    "        \"to_lowercase\": True,           # Uniformiser les cas pour éviter les variations inutiles.\n",
    "        \"remove_stopwords\": True,       # Les stopwords n'apportent aucune information pertinente.\n",
    "        \"lemmatize\": True,              # Simplifier les mots générés aléatoirement.\n",
    "    },\n",
    "    \"Syntax error\": {\n",
    "        \"remove_punctuation\": False,    # Garder la ponctuation pour identifier les erreurs syntaxiques.\n",
    "        \"to_lowercase\": True,           # Uniformiser les textes pour une analyse syntaxique cohérente.\n",
    "        \"remove_stopwords\": False,      # Les stopwords peuvent être nécessaires pour détecter les erreurs syntaxiques.\n",
    "        \"lemmatize\": False,             # Éviter de modifier les structures pour préserver l'intégrité syntaxique.\n",
    "    },\n",
    "    \"Contradiction\": {\n",
    "        \"remove_punctuation\": True,     # Supprimer la ponctuation pour se concentrer sur les contradictions sémantiques.\n",
    "        \"to_lowercase\": True,\n",
    "        \"remove_stopwords\": True,       # Les stopwords n'ont pas d'impact sur les contradictions.\n",
    "        \"lemmatize\": True,              # Faciliter la détection de contradictions.\n",
    "    },\n",
    "    \"Simple punctuation / grammar errors\": {\n",
    "        \"remove_punctuation\": False,    # Garder la ponctuation pour détecter les erreurs de ponctuation.\n",
    "        \"to_lowercase\": True,           # Uniformiser les cas.\n",
    "        \"remove_stopwords\": False,      # Conserver les stopwords pour ne pas altérer les phrases.\n",
    "        \"lemmatize\": False,             # Ne pas modifier les formes des mots.\n",
    "    },\n",
    "    \"Redundancy\": {\n",
    "        \"remove_punctuation\": True,     # Supprimer les ponctuations inutiles pour analyser la redondance.\n",
    "        \"to_lowercase\": True,           \n",
    "        \"remove_stopwords\": False,      # Conserver les stopwords pour identifier des répétitions complètes.\n",
    "        \"lemmatize\": False,             # Ne pas modifier les mots pour détecter les répétitions exactes.\n",
    "    },\n",
    "    \"Format misalignement\": {\n",
    "        \"remove_punctuation\": False,    # Garder la ponctuation pour vérifier les erreurs de format.\n",
    "        \"to_lowercase\": True,           \n",
    "        \"remove_stopwords\": False,      # Les stopwords peuvent être utiles pour détecter des problèmes de formatage.\n",
    "        \"lemmatize\": False,             \n",
    "    },\n",
    "    \"Prompt misalignement\": {\n",
    "        \"remove_punctuation\": False,     # Supprimer les caractères inutiles pour vérifier la correspondance avec le prompt.\n",
    "        \"to_lowercase\": True,\n",
    "        \"remove_stopwords\": True,       # Nettoyer les phrases pour se concentrer sur la correspondance avec le prompt.\n",
    "        \"lemmatize\": True,              \n",
    "    },\n",
    "    \"Out-of-Scope Generation\": {\n",
    "        \"remove_punctuation\": False,    # Garder la ponctuation pour analyser les phrases hors contexte.\n",
    "        \"to_lowercase\": True,           \n",
    "        \"remove_stopwords\": False,      \n",
    "        \"lemmatize\": True,              \n",
    "    },\n",
    "    \"Topic shift\": {\n",
    "        \"remove_punctuation\": False,   \n",
    "        \"to_lowercase\": True,\n",
    "        \"remove_stopwords\": False,    \n",
    "        \"lemmatize\": True,              \n",
    "    },\n",
    "    \"Oversimplification of Logical Arguments\": {\n",
    "        \"remove_punctuation\": True,     # Supprimer la ponctuation pour clarifier les arguments.\n",
    "        \"to_lowercase\": True,\n",
    "        \"remove_stopwords\": False,       \n",
    "        \"lemmatize\": False,              \n",
    "    },\n",
    "    \"Overgeneralization\": {\n",
    "        \"remove_punctuation\": True,     # Supprimer la ponctuation pour simplifier les phrases trop généralisées.\n",
    "        \"to_lowercase\": True,\n",
    "        \"remove_stopwords\": False,       \n",
    "        \"lemmatize\": False,              \n",
    "    },\n",
    "    \"Loss of Informative Content\": {\n",
    "        \"remove_punctuation\": True,     # Supprimer les ponctuations inutiles pour clarifier les informations perdues.\n",
    "        \"to_lowercase\": True,\n",
    "        \"remove_stopwords\": False,       \n",
    "        \"lemmatize\": False,              \n",
    "    },\n",
    "    \"Factuality hallucination\": {\n",
    "        \"remove_punctuation\": True,     # Supprimer les ponctuations inutiles pour analyser les faits.\n",
    "        \"to_lowercase\": True,\n",
    "        \"remove_stopwords\": False,       \n",
    "        \"lemmatize\": True,              \n",
    "    },\n",
    "    \"Faithfulness hallucination\": {\n",
    "        \"remove_punctuation\": True,     # Supprimer la ponctuation pour se concentrer sur la fidélité.\n",
    "        \"to_lowercase\": True,\n",
    "        \"remove_stopwords\": False,       \n",
    "        \"lemmatize\": True,              \n",
    "    },\n",
    "}\n",
    "\n",
    "\n",
    "# Liste des colonnes d'erreurs\n",
    "error_columns = [\n",
    "    'Random generation', 'Syntax error', 'Contradiction',\n",
    "    'Simple punctuation / grammar errors', 'Redundancy', 'Format misalignement',\n",
    "    'Prompt misalignement', 'Out-of-Scope Generation', 'Topic shift',\n",
    "    'Oversimplification of Logical Arguments', 'Overgeneralization',\n",
    "    'Loss of Informative Content', 'Factuality hallucination', 'Faithfulness hallucination'\n",
    "]\n",
    "\n",
    "# Demander à l'utilisateur de choisir une colonne cible\n",
    "print(\"Voici les types d'erreurs disponibles :\")\n",
    "for i, col in enumerate(error_columns):\n",
    "    print(f\"{i + 1}. {col}\")\n",
    "\n",
    "choice = int(input(\"Entrez le numéro correspondant à la cible souhaitée : \")) - 1\n",
    "target_column = error_columns[choice]\n",
    "\n",
    "print(f\"Vous avez choisi : {target_column}\")\n",
    "\n",
    "# Définir la colonne cible\n",
    "data['target'] = data[target_column].astype(int)\n",
    "\n",
    "# Appliquer le pipeline de nettoyage\n",
    "config = pipeline_configs[target_column]\n",
    "data['cleaned_source'] = data['source sentence'].apply(lambda x: clean_text(x, config))\n",
    "data['cleaned_simplified'] = data['simplified sentence'].apply(lambda x: clean_text(x, config))\n",
    "\n",
    "# Charger le modèle d'embedding\n",
    "model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "\n",
    "# Générer les embeddings\n",
    "data['source_embedding'] = data['cleaned_source'].apply(model.encode)\n",
    "data['simplified_embedding'] = data['cleaned_simplified'].apply(model.encode)\n",
    "\n",
    "# Calculer la similarité cosinus\n",
    "data['similarity'] = data.apply(\n",
    "    lambda row: cosine_similarity([row['source_embedding']], [row['simplified_embedding']])[0][0], axis=1\n",
    ")\n",
    "\n",
    "# Combiner les embeddings et la similarité\n",
    "X = np.array([\n",
    "    np.concatenate([row['source_embedding'], row['simplified_embedding'], [row['similarity']]])\n",
    "    for _, row in data.iterrows()\n",
    "])\n",
    "y = data['target'].values\n",
    "\n",
    "# Diviser les données en ensembles d'entraînement et de test\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "smote = SMOTE(random_state=42)\n",
    "X_train_resampled, y_train_resampled = smote.fit_resample(X_train, y_train)\n",
    "\n",
    "# Entraîner un modèle (Logistic Regression par exemple)\n",
    "logistic_model = LogisticRegression(max_iter=1000, random_state=42)\n",
    "logistic_model.fit(X_train_resampled, y_train_resampled)\n",
    "\n",
    "# Prédictions\n",
    "y_pred = logistic_model.predict(X_test)\n",
    "\n",
    "# Évaluation des performances\n",
    "print(\"\\nConfusion Matrix of logistic regression:\")\n",
    "print(confusion_matrix(y_test, y_pred))\n",
    "\n",
    "print(\"\\nClassification Report of logistic regression  :\")\n",
    "print(classification_report(y_test, y_pred, target_names=[\"False\", \"True\"]))\n",
    "print(f\"The balanced accuracy of the logistic_model logistic regression is \"\n",
    "      f\"{balanced_accuracy_score(y_test, y_pred):.3f}\")\n",
    "\n",
    "\n",
    "svm_model = SVC(kernel='linear', random_state=42)\n",
    "svm_model.fit(X_train_resampled, y_train_resampled)\n",
    "y_pred = svm_model.predict(X_test)\n",
    "print(\"Confusion Matrix of the model SVM :\")\n",
    "print(confusion_matrix(y_test, y_pred))\n",
    "\n",
    "print(\"\\nClassification Report of the model SVM :\")\n",
    "print(classification_report(y_test, y_pred, target_names=[\"False\", \"True\"]))\n",
    "print(f\"The balanced accuracy of the model SVM  is  \"\n",
    "      f\"{balanced_accuracy_score(y_test, y_pred):.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "L'erreur 'Random generation' est présente dans cette simplification.\n"
     ]
    }
   ],
   "source": [
    "# Fonction pour prédire si une erreur est présente ou non\n",
    "def predict_error(user_source, user_simplified, logistic_model, config):\n",
    "    # Nettoyer les phrases avec le pipeline configuré\n",
    "    cleaned_source = clean_text(user_source, config)\n",
    "    cleaned_simplified = clean_text(user_simplified, config)\n",
    "\n",
    "    # Générer les embeddings avec le modèle d'embedding\n",
    "    source_embedding = model.encode(cleaned_source)\n",
    "    simplified_embedding = model.encode(cleaned_simplified)\n",
    "\n",
    "    # Si les embeddings retournent un dictionnaire, extraire les vecteurs denses\n",
    "    if isinstance(source_embedding, dict) and 'dense_vecs' in source_embedding:\n",
    "        source_embedding = np.array(source_embedding['dense_vecs'])\n",
    "    if isinstance(simplified_embedding, dict) and 'dense_vecs' in simplified_embedding:\n",
    "        simplified_embedding = np.array(simplified_embedding['dense_vecs'])\n",
    "\n",
    "    # Calculer la similarité cosinus\n",
    "    similarity = cosine_similarity([source_embedding], [simplified_embedding])[0][0]\n",
    "\n",
    "    # Combiner les embeddings et la similarité en un seul vecteur\n",
    "    combined_features = np.concatenate([source_embedding, simplified_embedding, [similarity]])\n",
    "\n",
    "    # Prédire à l'aide du modèle entraîné\n",
    "    prediction = logistic_model.predict([combined_features])\n",
    "    return prediction[0]\n",
    "\n",
    "# Demander à l'utilisateur de saisir des phrases\n",
    "user_source = input(\"Entrez la phrase source : \")\n",
    "user_simplified = input(\"Entrez la phrase simplifiée : \")\n",
    "\n",
    "# Prédire le type d'erreur\n",
    "user_prediction = predict_error(user_source, user_simplified, logistic_model, config)\n",
    "\n",
    "# Afficher le résultat\n",
    "if user_prediction == 1:\n",
    "    print(f\"L'erreur '{target_column}' est présente dans cette simplification.\")\n",
    "else:\n",
    "    print(f\"L'erreur '{target_column}' n'est pas présente dans cette simplification.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
